{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Model1\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Transformations\n",
    "train_transforms = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                    ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ])\n",
    "\n",
    "#Creating the Datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transforms)\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                    download=True, transform=test_transforms)\n",
    "\n",
    "#Data Loaders\n",
    "batch_size = 2500\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                        shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                        shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(model, lossfunc, trainloader, valloader = None, scorefuncs = None, epochs = 100, device = \"cuda:0\", checkpoint_file = None):\n",
    "    \n",
    "    #Model -> the model to train\n",
    "    # Loss function -> the lost function that takes in two arguemnts, the model outputs and the labels and returns a score\n",
    "    # Train loader -> the PyTorch DataLoader object\n",
    "    # valloader -> an optional DataLoader for validation after every epoch\n",
    "    # scorefuncs -> dictionary of scoring functions to evaluate the performance\n",
    "    # epochs -> number of epochs to perform\n",
    "    #device -> the location of which to perform computation\n",
    "    \n",
    "    model = model()\n",
    "    tracking = [\"epoch\", \"total time\", \"train loss\", \"total correct\"]\n",
    "    if valloader is not None:\n",
    "        tracking.append(\"val loss\")\n",
    "        \n",
    "    for evalscore in scorefuncs:\n",
    "        tracking.append(\"train\" + evalscore)\n",
    "        if valloader is not None:\n",
    "            tracking.append(\"val\" + evalscore)\n",
    "            \n",
    "    totaltraintime = 0 # The time spent in the training loop\n",
    "    results = {}\n",
    "    \n",
    "    #initialising every item with an empty list\n",
    "    for item in tracking:\n",
    "        results[item] = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, betas= (0.9, 0.999), eps = 1e-08, weight_decay=0)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model = model.train()\n",
    "        total_correct = 0\n",
    "        total_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        start = time.time()\n",
    "        for inputs, labels in enumerate(trainloader, 0):\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            batch_size = inputs.shape[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(inputs).cuda()\n",
    "            \n",
    "            loss = lossfunc(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_correct += output.argmax(dim=1).eq(labels).sum().item()\n",
    "            \n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                y_true.append(labels[i])\n",
    "                y_pred.append(output[i,:])\n",
    "        end = time.time()\n",
    "        totaltraintime += (end-start)\n",
    "        \n",
    "        results[\"epoch\"].append(epoch)\n",
    "        results[\"total time\"].append(totaltraintime)\n",
    "        results[\"train loss\"].append(total_loss)\n",
    "        results[\"total correct\"].append(total_correct)\n",
    "        \n",
    "        y_pred = np.asarray(y_pred)\n",
    "        \n",
    "        if y_pred.shape[1] >1:\n",
    "            y_pred = np.argmax(y_pred, axis = 1)\n",
    "            \n",
    "        for name, scorefunc in scorefuncs.items():\n",
    "            results[\"train \" + name].append(scorefunc(y_true, y_pred))\n",
    "            \n",
    "        if valloader is None:\n",
    "            pass\n",
    "        else:\n",
    "            #validation performance\n",
    "            model = model.eval() #setting to evaluation  mode\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            running_loss = 0.0\n",
    "            total_correct = 0\n",
    "            \n",
    "            for inputs, labels in enumerate(valloader, 0):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                output = model(inputs)\n",
    "                \n",
    "                loss = lossfunc(output, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_correct += output.argmax(dim=1).eq(labels).sum().item()\n",
    "                \n",
    "                labels = labels.detach().cpu().numpy()\n",
    "                output = output.detach().cpu().numpy()\n",
    "                for i in range(batch_size):\n",
    "                    y_true.append(labels[i])\n",
    "                    y_pred.append(output[i,:])\n",
    "                \n",
    "            results[\"val loss\"].append(total_loss)\n",
    "            y_pred = np.asarray(y_pred)\n",
    "            \n",
    "            if y_pred.shape[1] >1:\n",
    "                y_pred = np.argmax(y_pred, axis =1)\n",
    "                \n",
    "            for name, scorefunc in scorefuncs.items():\n",
    "                results[\"val \" + name].append(scorefunc(y_true, y_pred))\n",
    "        if checkpoint_file is not None:\n",
    "            torch.save({\n",
    "                'epoch' : epoch,\n",
    "                'model_state_dict' : model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'results' : results \n",
    "            }, checkpoint_file)    \n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m lossfunc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m Model1\n\u001b[1;32m----> 3\u001b[0m model1Results \u001b[39m=\u001b[39m trainNetwork(model, lossfunc, trainloader, valloader\u001b[39m=\u001b[39;49mtrainloader, scorefuncs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mAccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m: accuracy_score})\n",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m, in \u001b[0;36mtrainNetwork\u001b[1;34m(model, lossfunc, trainloader, valloader, scorefuncs, epochs, device, checkpoint_file)\u001b[0m\n\u001b[0;32m     40\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m---> 43\u001b[0m     inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39;49mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m     batch_size \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     47\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "lossfunc = nn.CrossEntropyLoss()\n",
    "model = Model1\n",
    "model1Results = trainNetwork(model, lossfunc, trainloader, valloader=trainloader, scorefuncs={\"Accuracy\": accuracy_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y ='train Accuracy', data = model1Results, label = 'Train')\n",
    "sns.lineplot(x ='epoch',y = 'val Accuracy', data = model1Results, label = 'Validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
