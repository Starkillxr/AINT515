{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model1\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from models import Model1\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(model, lossfunc, trainloader, valloader = None, scorefuncs = None, epochs = 100, device = \"cuda:0\", checkpoint_file = None):\n",
    "    \n",
    "    #Model -> the model to train\n",
    "    # Loss function -> the lost function that takes in two arguemnts, the model outputs and the labels and returns a score\n",
    "    # Train loader -> the PyTorch DataLoader object\n",
    "    # valloader -> an optional DataLoader for validation after every epoch\n",
    "    # scorefuncs -> dictionary of scoring functions to evaluate the performance\n",
    "    # epochs -> number of epochs to perform\n",
    "    #device -> the location of which to perform computation\n",
    "    \n",
    "    tracking = [\"epoch\", \"total time\", \"train loss\", \"total correct\"]\n",
    "    if valloader is not None:\n",
    "        tracking.append(\"val loss\")\n",
    "        \n",
    "    for evalscore in scorefuncs:\n",
    "        tracking.append(\"train\" + evalscore)\n",
    "        if valloader is not None:\n",
    "            tracking.append(\"val\" + evalscore)\n",
    "            \n",
    "    totaltraintime = 0 # The time spent in the training loop\n",
    "    results = {}\n",
    "    \n",
    "    #initialising every item with an empty list\n",
    "    for item in tracking:\n",
    "        results[item] = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, betas= (0.9, 0.999), eps = 1e-08, weight_decay=0)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model = model.train()\n",
    "        total_correct = 0\n",
    "        total_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        start = time.time()\n",
    "        for inputs, labels in enumerate(trainloader, 0):\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            batch_size = inputs.shape[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(inputs).cuda()\n",
    "            \n",
    "            loss = lossfunc(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_correct += output.argmax(dim=1).eq(labels).sum().item()\n",
    "            \n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                y_true.append(labels[i])\n",
    "                y_pred.append(output[i,:])\n",
    "        end = time.time()\n",
    "        totaltraintime += (end-start)\n",
    "        \n",
    "        results[\"epoch\"].append(epoch)\n",
    "        results[\"total time\"].append(totaltraintime)\n",
    "        results[\"train loss\"].append(total_loss)\n",
    "        results[\"total correct\"].append(total_correct)\n",
    "        \n",
    "        y_pred = np.asarray(y_pred)\n",
    "        \n",
    "        if y_pred.shape[1] >1:\n",
    "            y_pred = np.argmax(y_pred, axis = 1)\n",
    "            \n",
    "        for name, scorefunc in scorefuncs.items():\n",
    "            results[\"train \" + name].append(scorefunc(y_true, y_pred))\n",
    "            \n",
    "        if valloader is None:\n",
    "            pass\n",
    "        else:\n",
    "            #validation performance\n",
    "            model = model.eval() #setting to evaluation  mode\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            running_loss = 0.0\n",
    "            total_correct = 0\n",
    "            \n",
    "            for inputs, labels in enumerate(valloader, 0):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                output = model(inputs)\n",
    "                \n",
    "                loss = lossfunc(output, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_correct += output.argmax(dim=1).eq(labels).sum().item()\n",
    "                \n",
    "                labels = labels.detach().cpu().numpy()\n",
    "                output = output.detach().cpu().numpy()\n",
    "                for i in range(batch_size):\n",
    "                    y_true.append(labels[i])\n",
    "                    y_pred.append(output[i,:])\n",
    "                \n",
    "            results[\"val loss\"].append(total_loss)\n",
    "            y_pred = np.asarray(y_pred)\n",
    "            \n",
    "            if y_pred.shape[1] >1:\n",
    "                y_pred = np.argmax(y_pred, axis =1)\n",
    "                \n",
    "            for name, scorefunc in scorefuncs.items():\n",
    "                results[\"val \" + name].append(scorefunc(y_true, y_pred))\n",
    "        if checkpoint_file is not None:\n",
    "            torch.save({\n",
    "                'epoch' : epoch,\n",
    "                'model_state_dict' : model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'results' : results \n",
    "            }, checkpoint_file)    \n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc = nn.CrossEntropyLoss()\n",
    "model1Results = trainNetwork(Model1, lossfunc, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
